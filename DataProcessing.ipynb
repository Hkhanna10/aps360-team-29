{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect, LangDetectException\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a value is a number\n",
    "def is_number(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Function to detect the language of the text\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except LangDetectException:\n",
    "        return 'unknown'\n",
    "    \n",
    "# Function to count the number of words in a text\n",
    "def word_count(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process FNC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredf\\AppData\\Local\\Temp\\ipykernel_3000\\4198228200.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_2['label'].fillna(df_3['label'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "df_1 = pd.read_csv('Data/Fake News Competition/train.csv')\n",
    "df_2 = pd.read_csv('Data/Fake News Competition/test.csv')\n",
    "df_3 = pd.read_csv('Data/Fake News Competition/submit.csv')\n",
    "\n",
    "df_2.set_index('id', inplace=True)\n",
    "df_2['label'] = np.nan\n",
    "df_3.set_index('id', inplace=True)\n",
    "\n",
    "df_2['label'].fillna(df_3['label'], inplace=True)\n",
    "df_2.reset_index(inplace=True)\n",
    "\n",
    "df = pd.concat([df_1, df_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no labels\n",
    "df_filtered = df[df['label'].apply(is_number)]\n",
    "\n",
    "# Remove rows without text\n",
    "df_filtered = df_filtered[~df_filtered['text'].isna()]\n",
    "\n",
    "# Remove non-english entry\n",
    "df_filtered['language'] = df_filtered['text'].apply(detect_language)\n",
    "df_filtered = df_filtered[df_filtered['language'] == 'en']\n",
    "\n",
    "# Remove rows where the 'text' column has fewer than 20 words\n",
    "df_filtered = df_filtered[df_filtered['text'].apply(word_count) >= 20]\n",
    "\n",
    "# Drop language column\n",
    "df_filtered_final = df_filtered.drop('language', axis=1)\n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "# df_filtered_final.to_csv('FNC_data_clean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 17361\n",
      "Validation set length: 3720\n",
      "Test set length: 3721\n"
     ]
    }
   ],
   "source": [
    "df_FNC_master = pd.read_csv('FNC_data_clean.csv')\n",
    "\n",
    "# Split data into 70% training and 30% temporary sets\n",
    "train_df, temp_df = train_test_split(df_FNC_master, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary set into 50% validation and 50% test sets (0.15 each of the original data)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the lengths of each set\n",
    "print(f'Training set length: {len(train_df)}')\n",
    "print(f'Validation set length: {len(val_df)}')\n",
    "print(f'Test set length: {len(test_df)}')\n",
    "\n",
    "# Save the dataset\n",
    "# train_df.to_csv('Model/Data_indi/FNC/train_df.csv')\n",
    "# val_df.to_csv('Model/Data_indi/FNC/val_df.csv')\n",
    "# test_df.to_csv('Model/Data_indi/FNC/test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing ISOT Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISOT dataset\n",
    "df_ISOT_True = pd.read_csv(\"Data/ISOT Dataset/True_manually_cleaned.csv\")\n",
    "df_ISOT_Fake = pd.read_csv(\"Data/ISOT Dataset/Fake_manually_cleaned_new.csv\")\n",
    "\n",
    "df_ISOT = pd.concat([df_ISOT_True, df_ISOT_Fake], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine ISOT and Fake News\n",
    "df_FNC_master_clean = df_FNC_master[['title', 'text', 'label']]\n",
    "df_ISOT_clean = df_ISOT[['title', 'text', 'label']]\n",
    "\n",
    "df_master = pd.concat([df_ISOT_clean, df_FNC_master_clean], axis=0)\n",
    "\n",
    "# Split data into 70% training and 30% temporary sets\n",
    "train_df, temp_df = train_test_split(df_master, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary set into 50% validation and 50% test sets (0.15 each of the original data)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the lengths of each set\n",
    "print(f'Training set length: {len(train_df)}')\n",
    "print(f'Validation set length: {len(val_df)}')\n",
    "print(f'Test set length: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_master.to_csv('FNC+ISOT_data_clean.csv', index=False)\n",
    "\n",
    "# Save the dataset\n",
    "train_df.to_csv('Model/Data_indi/ISOT/train_df.csv')\n",
    "val_df.to_csv('Model/Data_indi/ISOT/val_df.csv')\n",
    "test_df.to_csv('Model/Data_indi/ISOT/test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing FakeNewsCorpus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data in batch\n",
    "i = 0\n",
    "sampled_rows = []\n",
    "for chunk in pd.read_csv('Data/news_cleaned_2018_02_13.csv',chunksize = 50000, usecols= ['id', 'type', 'content', 'title'],\n",
    "                 lineterminator='\\n'):\n",
    "    i += 1\n",
    "    print(i)\n",
    "    sampled_chunk = chunk.sample(n=1000, random_state=i)\n",
    "    sampled_rows.append(sampled_chunk)\n",
    "\n",
    "sampled_df = pd.concat(sampled_rows, ignore_index=True) # Combine all sampled rows into a single DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "sampled_df.to_csv('news_data_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df = pd.read_csv('news_data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data classfication\n",
    "pd.unique(sampled_df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows without text\n",
    "df_news_filtered = sampled_df[~sampled_df['type'].isna()]\n",
    "df_news_filtered = df_news_filtered[~df_news_filtered['title'].isna()]\n",
    "df_news_filtered = df_news_filtered[~df_news_filtered['content'].isna()]\n",
    "\n",
    "# Normalise catagory\n",
    "label_mapping = {'fake': 1, \n",
    "                 'political': 2, \n",
    "                 'unreliable': 2, \n",
    "                 'conspiracy': 1, \n",
    "                 'unknown': 2,\n",
    "                 'bias': 2, \n",
    "                 'hate': 2, \n",
    "                 'junksci': 1, \n",
    "                 'reliable': 0, \n",
    "                 'clickbait': 2, \n",
    "                 'satire': 2,\n",
    "                 'rumor': 1}\n",
    "\n",
    "df_news_filtered['label'] = df_news_filtered['type'].replace(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the reliable entries\n",
    "df_news_filtered_fake = df_news_filtered[df_news_filtered['label'] == 1]\n",
    "# Remove non-english entry\n",
    "df_news_filtered_fake['language'] = df_news_filtered_fake['content'].apply(detect_language)\n",
    "df_news_filtered_fake = df_news_filtered_fake[df_news_filtered_fake['language'] == 'en']\n",
    "\n",
    "# Remove rows where the 'content' column has fewer than 20 words\n",
    "df_news_filtered_fake = df_news_filtered_fake[df_news_filtered_fake['content'].apply(word_count) >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fredf\\AppData\\Local\\Temp\\ipykernel_3000\\3891396704.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_news_filtered_reliable['language'] = df_news_filtered_reliable['content'].apply(detect_language)\n"
     ]
    }
   ],
   "source": [
    "# Extract the reliable entries\n",
    "df_news_filtered_reliable = df_news_filtered[df_news_filtered['label'] == 0]\n",
    "# Remove non-english entry\n",
    "df_news_filtered_reliable['language'] = df_news_filtered_reliable['content'].apply(detect_language)\n",
    "df_news_filtered_reliable = df_news_filtered_reliable[df_news_filtered_reliable['language'] == 'en']\n",
    "\n",
    "# Remove rows where the 'content' column has fewer than 20 words\n",
    "df_news_filtered_reliable = df_news_filtered_reliable[df_news_filtered_reliable['content'].apply(word_count) >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "df_news_filtered_reliable = df_news_filtered_reliable.rename(columns={'content': 'text'})\n",
    "df_news_filtered_fake = df_news_filtered_fake.rename(columns={'content': 'text'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 25,000 from both df\n",
    "df_news_filtered_reliable_select = df_news_filtered_reliable.sample(n=25000, random_state=42)\n",
    "df_news_filtered_fake_select = df_news_filtered_fake.sample(n=25000, random_state=42)\n",
    "\n",
    "# Only keep columns needed\n",
    "df_news_filtered_reliable_select_clean = df_news_filtered_reliable_select[['title', 'text', 'label']]\n",
    "df_news_filtered_fake_select_clean = df_news_filtered_fake_select[['title', 'text', 'label']]\n",
    "\n",
    "# Combine fake and reliable df\n",
    "df_news_clean = pd.concat([df_news_filtered_reliable_select_clean, df_news_filtered_fake_select_clean], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into 70% training and 30% temporary sets\n",
    "train_df, temp_df = train_test_split(df_news_clean, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary set into 50% validation and 50% test sets (0.15 each of the original data)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Verify the lengths of each set\n",
    "print(f'Training set length: {len(train_df)}')\n",
    "print(f'Validation set length: {len(val_df)}')\n",
    "print(f'Test set length: {len(test_df)}')\n",
    "\n",
    "# Save the dataset\n",
    "train_df.to_csv('Model/Data_indi/FNC/train_df.csv')\n",
    "val_df.to_csv('Model/Data_indi/FNC/val_df.csv')\n",
    "test_df.to_csv('Model/Data_indi/FNC/test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all df\n",
    "df_master = pd.concat([df_ISOT_clean, df_FNC_master_clean, df_news_clean], axis=0)\n",
    "\n",
    "df_master.replace('â€™', \"'\", regex=True, inplace=True)\n",
    "df_master.replace('â€˜', \"‘\", regex=True, inplace=True)\n",
    "df_master.replace('â€œ', \"“\", regex=True, inplace=True)\n",
    "df_master.replace('â€¦', \"...\", regex=True, inplace=True)\n",
    "df_master.replace('â€\\x9d', \"\", regex=True, inplace=True)\n",
    "\n",
    "# Split data into 70% training and 30% temporary sets\n",
    "train_df, temp_df = train_test_split(df_master, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the temporary set into 50% validation and 50% test sets (0.15 each of the original data)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write csv\n",
    "train_df.to_csv('train_df.csv', index=False)\n",
    "val_df.to_csv('val_df.csv', index=False)\n",
    "test_df.to_csv('test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data\n",
    "df_test = pd.read_csv('val_df_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
