# -*- coding: utf-8 -*-
"""baselinemodelhalfdatatuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lsAnV1ae75iJF8WMXnYC9es6X0mdLNUk
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.stats import randint
import string

#Dowload the stop words
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

#To tokonize
nltk.download('punkt')
from nltk.tokenize import word_tokenize

#Import the data
from google.colab import drive
drive.mount('/content/drive')
dataset_path = '/content/drive/My Drive/University/Second Year/APS360/APS360 - Project/APS 360 Project Group 29/Data/FNC_data_clean.csv'
dataset = pd.read_csv(dataset_path)

"""Prepare the data"""

def TextPreprocessing(text):
  #Turn the text to lower case
  text = text.lower()

  #Remove the words and punctuations which do not contribute to the text classification
  text = ''.join([c for c in text if c not in string.punctuation])
  word_tokens = word_tokenize(text)
  text = ''.join([w for w in word_tokens if not w in stop_words])
  return text

#Clean the data
dataset['text'] = dataset['text'].apply(TextPreprocessing)

"""Check the data"""

dataset.head()

"""Split the data"""

x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['label'], test_size=0.2)

"""Vecotize the data"""

tfidf_vectorizer = TfidfVectorizer()
x_train = tfidf_vectorizer.fit_transform(x_train)
x_test = tfidf_vectorizer.transform(x_test)

"""Model"""

help(RandomForestClassifier())

Model = RandomForestClassifier()
Model.fit(x_train, y_train)

"""Evaluate"""

y_pred = Model.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

"""Trying to tune the number of estimers and the depth"""

param_dist = {'n_estimators': randint(50,500),
              'max_depth': randint(1,100)}

# Use random search to find the best hyperparameters
rand_search = RandomizedSearchCV(Model,
                                 param_distributions = param_dist,
                                 n_iter=5,
                                 cv=5)

# Fit the random search object to the data
rand_search.fit(x_train, y_train)

best_model = rand_search.best_estimator_

# Print the best hyperparameters
print('Best hyperparameters:',  rand_search.best_params_)

Model_Best = RandomForestClassifier(n_estimators=226, max_depth=66)
Model_Best.fit(x_train, y_train)

y_pred = Model_Best.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

#Trying without a limit to the depth
Model_Best = RandomForestClassifier(n_estimators=226)
Model_Best.fit(x_train, y_train)

y_pred = Model_Best.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

Model_Best = RandomForestClassifier(n_estimators=95)
Model_Best.fit(x_train, y_train)

y_pred = Model_Best.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)